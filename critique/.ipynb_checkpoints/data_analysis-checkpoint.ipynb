{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5484e3d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T00:57:06.269195600Z",
     "start_time": "2023-11-21T00:57:05.814231Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2f17cd49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T00:57:06.454192900Z",
     "start_time": "2023-11-21T00:57:05.834249800Z"
    }
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d4d9eae5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T00:57:06.454192900Z",
     "start_time": "2023-11-21T00:57:06.188192600Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def convexhull(x, n_components):\n",
    "    pca = PCA(n_components = n_components)\n",
    "    reduced_data = pca.fit_transform(x)\n",
    "    explained_ratio = pca.explained_variance_ratio_\n",
    "    hull = ConvexHull(reduced_data)\n",
    "    volume = hull.volume\n",
    "    return volume, explained_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4ddc01c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T00:57:06.454192900Z",
     "start_time": "2023-11-21T00:57:06.204192400Z"
    }
   },
   "outputs": [],
   "source": [
    "def percent_change(values):\n",
    "    lst = []\n",
    "    for i in range(len(values)):\n",
    "        percent_change = ((values[i] - values[6])/values[6])*100\n",
    "        lst.append(round(percent_change))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f9e0475e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T00:57:06.490192800Z",
     "start_time": "2023-11-21T00:57:06.223195400Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def DPP_diversity(x, lambda0=0.1):\n",
    "    x = tf.convert_to_tensor(x, dtype='float32')\n",
    "    \n",
    "    r = tf.reduce_sum(tf.math.square(x), axis =1, keepdims = True)\n",
    "    D = r-2*tf.matmul(x, tf.transpose(x))+tf.transpose(r)\n",
    "    S = tf.exp(-0.5*tf.math.square(D))\n",
    "    y = tf.ones(np.shape(x)[0])\n",
    "    Q = tf.tensordot(tf.expand_dims(y, 1), tf.expand_dims(y, 0), 1)\n",
    "    if lambda0 == 0:\n",
    "        L = S\n",
    "    else:\n",
    "        L= S*tf.math.pow(Q, lambda0)\n",
    "    try:\n",
    "        eig_val, _  = tf.linalg.eigh(L)\n",
    "    except:\n",
    "        eig_val = tf.ones_like(y)\n",
    "    loss = -tf.reduce_mean(tf.math.log(tf.math.maximum(eig_val, 1e-7)))\n",
    "    return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8652a475",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T00:57:06.492192700Z",
     "start_time": "2023-11-21T00:57:06.237192900Z"
    }
   },
   "outputs": [],
   "source": [
    "def distance_to_centroid(embeddings):\n",
    "    distances = []\n",
    "    for i in range(embeddings.shape[0]):\n",
    "        pca = PCA(n_components = 20)\n",
    "        embeddings = pca.fit_transform(embeddings)\n",
    "        mean = np.mean(embeddings[i])\n",
    "        dist = np.sqrt(np.sum(np.square(np.subtract(embeddings[i], mean))))\n",
    "        distances.append(dist)\n",
    "    return np.mean(np.array(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5316e191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T00:57:06.492192700Z",
     "start_time": "2023-11-21T00:57:06.253192700Z"
    }
   },
   "outputs": [],
   "source": [
    "def L2_vectorized(X, Y):\n",
    "    #Vectorize L2 calculation using x^2+y^2-2xy\n",
    "    X_sq = np.sum(np.square(X), axis=1)\n",
    "    Y_sq = np.sum(np.square(Y), axis=1)\n",
    "    sq = np.add(np.expand_dims(X_sq, axis=-1), np.transpose(Y_sq)) - 2*np.matmul(X,np.transpose(Y))\n",
    "    sq = np.clip(sq, 0.0, 1e12)\n",
    "    return np.sqrt(sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7d92655e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T00:57:06.493192800Z",
     "start_time": "2023-11-21T00:57:06.268195600Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_distance(X, Y, distance=\"Euclidean\"):\n",
    "    if distance==\"Euclidean\":\n",
    "        return L2_vectorized(X,Y)\n",
    "    else:\n",
    "        raise Exception(\"Unknown distance metric specified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2dd6fdd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T00:57:06.493192800Z",
     "start_time": "2023-11-21T00:57:06.284193Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_gen_distance(embeddings, reduction):\n",
    "    x = embeddings\n",
    "    res = calc_distance(x, x, distance = \"Euclidean\")\n",
    "    # this sets the diagonal of the matrix to the maximum of elements across the column dimension (axis = 1)\n",
    "    res = tf.linalg.set_diag(res, tf.reduce_max(res, axis=1))\n",
    "    # pick the smallest values along the columns\n",
    "    if reduction == \"min\":\n",
    "        scores = tf.reduce_min(res, axis=1)\n",
    "    # pick the average value along the columns\n",
    "    elif reduction == \"ave\":\n",
    "        scores = tf.reduce_mean(res, axis=1)\n",
    "    else:\n",
    "        raise Exception(\"Unknown reduction method\")\n",
    "    return np.mean(scores.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a604ad",
   "metadata": {},
   "source": [
    "## Using critique method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ae432",
   "metadata": {},
   "source": [
    "## RQ2:\n",
    "\n",
    "(1b) How does styling of the input prompt impact the output quality and diversity of the prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f3bbf488",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T00:57:06.493192800Z",
     "start_time": "2023-11-21T00:57:06.299192900Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of CSV files\n",
    "csv_files = [\n",
    "    'data/critique_towels.csv',\n",
    "    'data/critique_powder.csv',\n",
    "    'data/critique_time.csv',\n",
    "    'data/critique_exercise.csv',\n",
    "    'data/critique_froth.csv',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "22055476",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T00:59:10.355325700Z",
     "start_time": "2023-11-21T00:57:06.316192400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:28<00:00, 28.28s/it]\u001b[A\n",
      " 20%|██        | 1/5 [00:28<01:53, 28.28s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:23<00:00, 23.14s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:51<01:15, 25.26s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:22<00:00, 22.00s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:13<00:47, 23.77s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:27<00:00, 27.20s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:40<00:25, 25.13s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:23<00:00, 23.37s/it]\u001b[A\n",
      "100%|██████████| 5/5 [02:04<00:00, 24.80s/it]\n"
     ]
    }
   ],
   "source": [
    "# this only needs to be run once.\n",
    "dict_1_DPP = {}\n",
    "dict_1_convex = {}\n",
    "dict_1_centroid = {}\n",
    "dict_1_nearest = {}\n",
    "for csv_file in tqdm(csv_files):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    count = 0\n",
    "    for column in tqdm(df.columns):\n",
    "        # encode the column text data into embeddings\n",
    "        embeddings = model.encode(df[column].astype(str).tolist())\n",
    "        # calculate the DPP\n",
    "        dict_1_DPP[(csv_file, count)] = DPP_diversity(embeddings, lambda0=0.1)\n",
    "        # calculate the convex hull\n",
    "        dict_1_convex[(csv_file, count)] = convexhull(embeddings, n_components = 13)\n",
    "        # calculate the distance to centroid\n",
    "        dict_1_centroid[(csv_file, count)] = distance_to_centroid(embeddings)\n",
    "        # calculate the nearest generated distance (average)\n",
    "        dict_1_nearest[(csv_file, count)] = gen_gen_distance(embeddings, reduction = \"ave\")\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "52b4fb30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T00:59:10.366326Z",
     "start_time": "2023-11-21T00:59:10.355325700Z"
    }
   },
   "outputs": [],
   "source": [
    "# DPP\n",
    "\n",
    "# convert tuples in dictionary to strings\n",
    "dict_1_DPP_str = {str(key): value for key, value in dict_1_DPP.items()}\n",
    "\n",
    "# convert float32 values to float\n",
    "dict_1_DPP_str = {key:float(value) for key, value in dict_1_DPP_str.items()}\n",
    "\n",
    "with open(\"data/DPP_Topics.json\", \"w\") as file:\n",
    "    json.dump(dict_1_DPP_str, file)\n",
    "    \n",
    "# Centroid Distance\n",
    "dict_1_centroid_str = {str(key): value for key, value in dict_1_centroid.items()}\n",
    "\n",
    "dict_1_centroid_str = {key:float(value) for key, value in dict_1_centroid_str.items()}\n",
    "\n",
    "with open(\"data/centroid_Topics.json\", \"w\") as file:\n",
    "    json.dump(dict_1_centroid_str, file)\n",
    "    \n",
    "# Nearest Generated Sample\n",
    "dict_1_nearest_str = {str(key): value for key, value in dict_1_nearest.items()}\n",
    "\n",
    "dict_1_nearest_str = {key:float(value) for key, value in dict_1_nearest_str.items()}\n",
    "\n",
    "with open(\"data/nearest_Topics.json\", \"w\") as file:\n",
    "    json.dump(dict_1_nearest_str, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "90438916",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T00:59:10.382325900Z",
     "start_time": "2023-11-21T00:59:10.361325800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convex Hull\n",
    "def convert_to_json(obj):\n",
    "    if isinstance(obj, tuple):\n",
    "        return {'__tuple__': True, 'items': list(obj)}\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return {'__ndarray__': True, 'n_component': obj.tolist()}\n",
    "    return obj\n",
    "\n",
    "\n",
    "dict_1_convex_str = {str(key): value for key, value in dict_1_convex.items()}\n",
    "\n",
    "with open(\"data/convex_Topics.json\", \"w\") as file:\n",
    "    json.dump(dict_1_convex_str, file, default = convert_to_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1dca3b11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T00:59:10.390326100Z",
     "start_time": "2023-11-21T00:59:10.377326200Z"
    }
   },
   "outputs": [],
   "source": [
    "# to retrieve the json file\n",
    "with open(\"data/DPP_Topics.json\", \"r\") as file:\n",
    "    dict_1_DPP_json = json.load(file)\n",
    "dict_1_DPP_json = {eval(key): value for key, value in dict_1_DPP_json.items()}\n",
    "\n",
    "with open(\"data/centroid_Topics.json\", \"r\") as file:\n",
    "    dict_1_centroid_json = json.load(file)\n",
    "dict_1_centroid_json = {eval(key): value for key, value in dict_1_centroid_json.items()}\n",
    "\n",
    "with open(\"data/nearest_Topics.json\", \"r\") as file:\n",
    "    dict_1_nearest_json = json.load(file)\n",
    "dict_1_nearest_json = {eval(key): value for key, value in dict_1_nearest_json.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e1289f66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T00:59:10.413325800Z",
     "start_time": "2023-11-21T00:59:10.390326100Z"
    }
   },
   "outputs": [],
   "source": [
    "# to convert convex hull back to the original dictionary with tuples and numpy array, we can use a custom decoder function\n",
    "\n",
    "def custom_decoder(obj):\n",
    "    if '__tuple__' in obj:\n",
    "        return tuple(obj['items'])\n",
    "    elif '__ndarray__' in obj:\n",
    "        return np.array(obj['n_component'])\n",
    "    return obj\n",
    "\n",
    "with open(\"data/convex_Topics.json\", \"r\") as file:\n",
    "    dict_1_convex_json = json.load(file)\n",
    "\n",
    "# Convert tuples and NumPy arrays back to original format\n",
    "dict_1_convex_json = {key: (value[0], value[1]['n_component']) for key, value in dict_1_convex_json.items()}\n",
    "\n",
    "# we can clean it up further by summing all the values in the n_components to get the sum of information retained\n",
    "dict_1_convex_json = {key: (value[0], sum(value[1])) for key, value in dict_1_convex_json.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0613a478",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T01:36:08.422927500Z",
     "start_time": "2023-11-21T01:36:08.401922700Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the previous json files as dict\n",
    "with open(\"../tempTopP-Adj/data/DPP_Topics.json\", \"r\") as file:\n",
    "    dict_2_DPP_json = json.load(file)\n",
    "dict_2_DPP_json = {eval(key): value for key, value in dict_2_DPP_json.items()}\n",
    "\n",
    "with open(\"../tempTopP-Adj/data/centroid_Topics.json\", \"r\") as file:\n",
    "    dict_2_centroid_json = json.load(file)\n",
    "dict_2_centroid_json = {eval(key): value for key, value in dict_2_centroid_json.items()}\n",
    "\n",
    "with open(\"../tempTopP-Adj/data/nearest_Topics.json\", \"r\") as file:\n",
    "    dict_2_nearest_json = json.load(file)\n",
    "dict_2_nearest_json = {eval(key): value for key, value in dict_2_nearest_json.items()}\n",
    "\n",
    "# to convert convex hull back to the original dictionary with tuples and numpy array, we can use a custom decoder function\n",
    "\n",
    "def custom_decoder(obj):\n",
    "    if '__tuple__' in obj:\n",
    "        return tuple(obj['items'])\n",
    "    elif '__ndarray__' in obj:\n",
    "        return np.array(obj['n_component'])\n",
    "    return obj\n",
    "\n",
    "with open(\"../tempTopP-Adj/data/convex_Topics.json\", \"r\") as file:\n",
    "    dict_2_convex_json = json.load(file)\n",
    "\n",
    "# Convert tuples and NumPy arrays back to original format\n",
    "dict_2_convex_json = {key: (value[0], value[1]['n_component']) for key, value in dict_2_convex_json.items()}\n",
    "\n",
    "# we can clean it up further by summing all the values in the n_components to get the sum of information retained\n",
    "dict_2_convex_json = {key: (value[0], sum(value[1])) for key, value in dict_2_convex_json.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9c81e805",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T01:03:46.627974800Z",
     "start_time": "2023-11-21T01:03:46.603024300Z"
    }
   },
   "outputs": [],
   "source": [
    "# DPP percent difference calculation\n",
    "percent_diff = {}\n",
    "for key, value in dict_2_DPP_json.items():\n",
    "    csv_file = key[0]\n",
    "    if csv_file not in percent_diff:\n",
    "        percent_diff[csv_file] = []\n",
    "    percent_diff[csv_file].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fc29e2c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T01:03:47.493306Z",
     "start_time": "2023-11-21T01:03:47.463306Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e77d4cd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T01:04:22.149720100Z",
     "start_time": "2023-11-21T01:04:22.105671300Z"
    }
   },
   "outputs": [],
   "source": [
    "# append values\n",
    "percent_diff['data/ablation_topic_towels.csv'].append(dict_1_DPP_json[('data/critique_towels.csv', 0)])\n",
    "percent_diff['data/ablation_topic_powder.csv'].append(dict_1_DPP_json[('data/critique_powder.csv', 0)])\n",
    "percent_diff['data/ablation_topic_time.csv'].append(dict_1_DPP_json[('data/critique_time.csv', 0)])\n",
    "percent_diff['data/ablation_topic_exercise.csv'].append(dict_1_DPP_json[('data/critique_exercise.csv', 0)])\n",
    "percent_diff['data/ablation_topic_froth.csv'].append(dict_1_DPP_json[('data/critique_froth.csv', 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "272a0db2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T01:08:09.104942Z",
     "start_time": "2023-11-21T01:08:09.081944600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note order goes zero-shot, few-shot, novel, unique, creative, human-1, human-2, critique\n",
      "\n",
      "Percent difference for data/ablation_topic_towels.csv:[17, -1, 7, 23, -8, -3, 0, -8]\n",
      "Percent difference for data/ablation_topic_powder.csv:[10, 37, 32, 32, 10, 6, 0, -16]\n",
      "Percent difference for data/ablation_topic_time.csv:[51, 16, 31, 58, 71, -2, 0, 31]\n",
      "Percent difference for data/ablation_topic_exercise.csv:[30, 21, 5, 41, 13, 7, 0, -15]\n",
      "Percent difference for data/ablation_topic_froth.csv:[8, 11, 1, -13, -1, 1, 0, -19]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Note order goes zero-shot, few-shot, novel, unique, creative, human-1, human-2, critique\")\n",
    "print(\"\")\n",
    "for csv_file, values in percent_diff.items():\n",
    "    percent_changes = percent_change(values)\n",
    "    print(f\"Percent difference for {csv_file}:{percent_changes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "aff1fdf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T01:34:50.542246600Z",
     "start_time": "2023-11-21T01:34:50.518191600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note order goes zero-shot, few-shot, novel, unique, creative, human-1, human-2, critique\n",
      "\n",
      "Percent difference for data/ablation_topic_towels.csv:[-24, -15, -17, -24, -10, 3, 0, -8]\n",
      "Percent difference for data/ablation_topic_powder.csv:[-7, -11, -10, -10, -7, 0, 0, 1]\n",
      "Percent difference for data/ablation_topic_time.csv:[-16, -9, -11, -11, -16, 1, 0, -13]\n",
      "Percent difference for data/ablation_topic_exercise.csv:[-4, -4, -2, -5, -4, -2, 0, -2]\n",
      "Percent difference for data/ablation_topic_froth.csv:[-7, -8, -7, -9, -7, -2, 0, -14]\n"
     ]
    }
   ],
   "source": [
    "# Nearest percent difference calculation\n",
    "percent_diff = {}\n",
    "for key, value in dict_2_nearest_json.items():\n",
    "    csv_file = key[0]\n",
    "    if csv_file not in percent_diff:\n",
    "        percent_diff[csv_file] = []\n",
    "    percent_diff[csv_file].append(value)\n",
    "\n",
    "# append values\n",
    "percent_diff['data/ablation_topic_towels.csv'].append(dict_1_nearest_json[('data/critique_towels.csv', 0)])\n",
    "percent_diff['data/ablation_topic_powder.csv'].append(dict_1_nearest_json[('data/critique_powder.csv', 0)])\n",
    "percent_diff['data/ablation_topic_time.csv'].append(dict_1_nearest_json[('data/critique_time.csv', 0)])\n",
    "percent_diff['data/ablation_topic_exercise.csv'].append(dict_1_nearest_json[('data/critique_exercise.csv', 0)])\n",
    "percent_diff['data/ablation_topic_froth.csv'].append(dict_1_nearest_json[('data/critique_froth.csv', 0)])\n",
    "\n",
    "\n",
    "print(\"Note order goes zero-shot, few-shot, novel, unique, creative, human-1, human-2, critique\")\n",
    "print(\"\")\n",
    "for csv_file, values in percent_diff.items():\n",
    "    percent_changes = percent_change(values)\n",
    "    print(f\"Percent difference for {csv_file}:{percent_changes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ca2f8b56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T01:37:19.149000300Z",
     "start_time": "2023-11-21T01:37:19.120992100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note order goes zero-shot, few-shot, novel, unique, creative, human-1, human-2, critique\n",
      "\n",
      "Percent difference for data/ablation_topic_towels.csv:[-98, -94, -95, -98, -84, 26, 0, -82]\n",
      "Percent difference for data/ablation_topic_powder.csv:[-72, -84, -77, -80, -77, 32, 0, -27]\n",
      "Percent difference for data/ablation_topic_time.csv:[-95, -84, -86, -86, -95, -12, 0, -93]\n",
      "Percent difference for data/ablation_topic_exercise.csv:[-46, -46, -45, -55, -54, -19, 0, -54]\n",
      "Percent difference for data/ablation_topic_froth.csv:[-86, -83, -86, -65, -88, -33, 0, -83]\n"
     ]
    }
   ],
   "source": [
    "# Convex Hull percent difference calculation\n",
    "percent_diff = {}\n",
    "for key, value in dict_2_convex_json.items():\n",
    "    key = eval(key)\n",
    "    csv_file = key[0]\n",
    "    value1 = value[0]\n",
    "    if csv_file not in percent_diff:\n",
    "        percent_diff[csv_file] = []\n",
    "    percent_diff[csv_file].append(value1)\n",
    "\n",
    "# append values\n",
    "percent_diff['data/ablation_topic_towels.csv'].append(dict_1_convex_json[\"('data/critique_towels.csv', 0)\"][0])\n",
    "percent_diff['data/ablation_topic_powder.csv'].append(dict_1_convex_json[\"('data/critique_powder.csv', 0)\"][0])\n",
    "percent_diff['data/ablation_topic_time.csv'].append(dict_1_convex_json[\"('data/critique_time.csv', 0)\"][0])\n",
    "percent_diff['data/ablation_topic_exercise.csv'].append(dict_1_convex_json[\"('data/critique_exercise.csv', 0)\"][0])\n",
    "percent_diff['data/ablation_topic_froth.csv'].append(dict_1_convex_json[\"('data/critique_froth.csv', 0)\"][0])\n",
    "\n",
    "\n",
    "print(\"Note order goes zero-shot, few-shot, novel, unique, creative, human-1, human-2, critique\")\n",
    "print(\"\")\n",
    "for csv_file, values in percent_diff.items():\n",
    "    percent_changes = percent_change(values)\n",
    "    print(f\"Percent difference for {csv_file}:{percent_changes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cc98957b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T01:38:23.347127400Z",
     "start_time": "2023-11-21T01:38:23.313067600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note order goes zero-shot, few-shot, novel, unique, creative, human-1, human-2, critique\n",
      "\n",
      "Percent difference for data/ablation_topic_towels.csv:[-27, -19, -20, -27, -13, 2, 0, -11]\n",
      "Percent difference for data/ablation_topic_powder.csv:[-8, -10, -10, -11, -8, 1, 0, 1]\n",
      "Percent difference for data/ablation_topic_time.csv:[-17, -10, -11, -11, -16, 1, 0, -15]\n",
      "Percent difference for data/ablation_topic_exercise.csv:[-3, -4, -2, -4, -3, -1, 0, -3]\n",
      "Percent difference for data/ablation_topic_froth.csv:[-7, -8, -8, -15, -8, -2, 0, -18]\n"
     ]
    }
   ],
   "source": [
    "# Centroid Distance percent difference calculation\n",
    "percent_diff = {}\n",
    "for key, value in dict_2_centroid_json.items():\n",
    "    csv_file = key[0]\n",
    "    if csv_file not in percent_diff:\n",
    "        percent_diff[csv_file] = []\n",
    "    percent_diff[csv_file].append(value)\n",
    "\n",
    "# append values\n",
    "percent_diff['data/ablation_topic_towels.csv'].append(dict_1_centroid_json[('data/critique_towels.csv', 0)])\n",
    "percent_diff['data/ablation_topic_powder.csv'].append(dict_1_centroid_json[('data/critique_powder.csv', 0)])\n",
    "percent_diff['data/ablation_topic_time.csv'].append(dict_1_centroid_json[('data/critique_time.csv', 0)])\n",
    "percent_diff['data/ablation_topic_exercise.csv'].append(dict_1_centroid_json[('data/critique_exercise.csv', 0)])\n",
    "percent_diff['data/ablation_topic_froth.csv'].append(dict_1_centroid_json[('data/critique_froth.csv', 0)])\n",
    "\n",
    "\n",
    "print(\"Note order goes zero-shot, few-shot, novel, unique, creative, human-1, human-2, critique\")\n",
    "print(\"\")\n",
    "for csv_file, values in percent_diff.items():\n",
    "    percent_changes = percent_change(values)\n",
    "    print(f\"Percent difference for {csv_file}:{percent_changes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f6995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
