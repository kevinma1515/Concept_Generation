{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5484e3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\anaconda3\\envs\\research\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f17cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d9eae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.decomposition import PCA\n",
    "def convexhull(x, n_components):\n",
    "    pca = PCA(n_components = n_components)\n",
    "    reduced_data = pca.fit_transform(x)\n",
    "    explained_ratio = pca.explained_variance_ratio_\n",
    "    hull = ConvexHull(reduced_data)\n",
    "    volume = hull.volume\n",
    "    return volume, explained_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ddc01c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_change(values):\n",
    "    lst = []\n",
    "    for i in range(len(values)):\n",
    "        percent_change = ((values[i] - values[0])/values[0])*100\n",
    "        lst.append(round(percent_change))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9e0475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def DPP_diversity(x, lambda0=0.1):\n",
    "    x = tf.convert_to_tensor(x, dtype='float32')\n",
    "    \n",
    "    r = tf.reduce_sum(tf.math.square(x), axis =1, keepdims = True)\n",
    "    D = r-2*tf.matmul(x, tf.transpose(x))+tf.transpose(r)\n",
    "    S = tf.exp(-0.5*tf.math.square(D))\n",
    "    y = tf.ones(np.shape(x)[0])\n",
    "    Q = tf.tensordot(tf.expand_dims(y, 1), tf.expand_dims(y, 0), 1)\n",
    "    if lambda0 == 0:\n",
    "        L = S\n",
    "    else:\n",
    "        L= S*tf.math.pow(Q, lambda0)\n",
    "    try:\n",
    "        eig_val, _  = tf.linalg.eigh(L)\n",
    "    except:\n",
    "        eig_val = tf.ones_like(y)\n",
    "    loss = -tf.reduce_mean(tf.math.log(tf.math.maximum(eig_val, 1e-7)))\n",
    "    return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8652a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_centroid(embeddings):\n",
    "    distances = []\n",
    "    for i in range(embeddings.shape[0]):\n",
    "        pca = PCA(n_components = 20)\n",
    "        embeddings = pca.fit_transform(embeddings)\n",
    "        mean = np.mean(embeddings[i])\n",
    "        dist = np.sqrt(np.sum(np.square(np.subtract(embeddings[i], mean))))\n",
    "        distances.append(dist)\n",
    "    return np.mean(np.array(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5316e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_vectorized(X, Y):\n",
    "    #Vectorize L2 calculation using x^2+y^2-2xy\n",
    "    X_sq = np.sum(np.square(X), axis=1)\n",
    "    Y_sq = np.sum(np.square(Y), axis=1)\n",
    "    sq = np.add(np.expand_dims(X_sq, axis=-1), np.transpose(Y_sq)) - 2*np.matmul(X,np.transpose(Y))\n",
    "    sq = np.clip(sq, 0.0, 1e12)\n",
    "    return np.sqrt(sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d92655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(X, Y, distance=\"Euclidean\"):\n",
    "    if distance==\"Euclidean\":\n",
    "        return L2_vectorized(X,Y)\n",
    "    else:\n",
    "        raise Exception(\"Unknown distance metric specified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dd6fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_gen_distance(embeddings, reduction):\n",
    "    x = embeddings\n",
    "    res = calc_distance(x, x, distance = \"Euclidean\")\n",
    "    # this sets the diagonal of the matrix to the maximum of elements across the column dimension (axis = 1)\n",
    "    res = tf.linalg.set_diag(res, tf.reduce_max(res, axis=1))\n",
    "    # pick the smallest values along the columns\n",
    "    if reduction == \"min\":\n",
    "        scores = tf.reduce_min(res, axis=1)\n",
    "    # pick the average value along the columns\n",
    "    elif reduction == \"ave\":\n",
    "        scores = tf.reduce_mean(res, axis=1)\n",
    "    else:\n",
    "        raise Exception(\"Unknown reduction method\")\n",
    "    return np.mean(scores.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e344e885",
   "metadata": {},
   "source": [
    "## RQ1:\n",
    "\n",
    "(1a) How do parameters such as temperature and Top P affect the quality and diversity of the generated text output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "554beefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of CSV files\n",
    "csv_files = [\n",
    "    'data/ablation_tempTopP_froth.csv',\n",
    "    'data/ablation_tempTopP_towels.csv',\n",
    "    'data/ablation_tempTopP_time.csv',\n",
    "    'data/ablation_tempTopP_powder.csv',\n",
    "    'data/ablation_tempTopP_exercise.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99004e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this only needs to be ran once.\n",
    "dict_1_DPP = {}\n",
    "dict_1_convex = {}\n",
    "dict_1_centroid = {}\n",
    "dict_1_nearest = {}\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    count = 0\n",
    "    for column in df.columns:\n",
    "        # encode the column text data into embeddings\n",
    "        embeddings = model.encode(df[column].astype(str).tolist())\n",
    "        # calculate the DPP\n",
    "        dict_1_DPP[(csv_file, count)] = DPP_diversity(embeddings, lambda0=0.1)\n",
    "        # calculate the convex hull\n",
    "        dict_1_convex[(csv_file, count)] = convexhull(embeddings, n_components = 13)\n",
    "        # calculate the distance to centroid\n",
    "        dict_1_centroid[(csv_file, count)] = distance_to_centroid(embeddings)\n",
    "        # calculate the nearest generated distance (average)\n",
    "        dict_1_nearest[(csv_file, count)] = gen_gen_distance(embeddings, reduction = \"ave\")\n",
    "        count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dec92fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dictionary results into a json file\n",
    "import json\n",
    "# DPP\n",
    "# convert tuples in dictionary to strings\n",
    "dict_1_DPP_str = {str(key): value for key, value in dict_1_DPP.items()}\n",
    "\n",
    "# convert float32 values to float\n",
    "dict_1_DPP_str = {key:float(value) for key, value in dict_1_DPP_str.items()}\n",
    "\n",
    "with open(\"DPP_TempTopP.json\", \"w\") as file:\n",
    "    json.dump(dict_1_DPP_str, file)\n",
    "    \n",
    "# Centroid Distance\n",
    "dict_1_centroid_str = {str(key): value for key, value in dict_1_centroid.items()}\n",
    "\n",
    "dict_1_centroid_str = {key:float(value) for key, value in dict_1_centroid_str.items()}\n",
    "\n",
    "with open(\"centroid_TempTopP.json\", \"w\") as file:\n",
    "    json.dump(dict_1_centroid_str, file)\n",
    "    \n",
    "# Nearest Generated Sample\n",
    "dict_1_nearest_str = {str(key): value for key, value in dict_1_nearest.items()}\n",
    "\n",
    "dict_1_nearest_str = {key:float(value) for key, value in dict_1_nearest_str.items()}\n",
    "\n",
    "with open(\"nearest_TempTopP.json\", \"w\") as file:\n",
    "    json.dump(dict_1_nearest_str, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6db62273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to retrieve the json file\n",
    "with open(\"DPP_TempTopP.json\", \"r\") as file:\n",
    "    dict_1_DPP_json = json.load(file)\n",
    "dict_1_DPP_json = {eval(key): value for key, value in dict_1_DPP_json.items()}\n",
    "\n",
    "with open(\"centroid_TempTopP.json\", \"r\") as file:\n",
    "    dict_1_centroid_json = json.load(file)\n",
    "dict_1_centroid_json = {eval(key): value for key, value in dict_1_centroid_json.items()}\n",
    "\n",
    "with open(\"nearest_TempTopP.json\", \"r\") as file:\n",
    "    dict_1_nearest_json = json.load(file)\n",
    "dict_1_nearest_json = {eval(key): value for key, value in dict_1_nearest_json.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "53d1591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convex hull is a bit trickier...\n",
    "\n",
    "# Convert tuples and arrays to compatible format\n",
    "# custom conversion function that converts tuples to a dictionary with a special key and arrays\n",
    "# to a dictionary with a special key\n",
    "def convert_to_json(obj):\n",
    "    if isinstance(obj, tuple):\n",
    "        return {'__tuple__': True, 'items': list(obj)}\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return {'__ndarray__': True, 'n_component': obj.tolist()}\n",
    "    return obj\n",
    "\n",
    "\n",
    "dict_1_convex_str = {str(key): value for key, value in dict_1_convex.items()}\n",
    "\n",
    "with open(\"convex_TempTopP.json\", \"w\") as file:\n",
    "    json.dump(dict_1_convex_str, file, default = convert_to_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "698b7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert convex hull back to the original dictionary with tuples and numpy array, we can use a custom decoder function\n",
    "\n",
    "def custom_decoder(obj):\n",
    "    if '__tuple__' in obj:\n",
    "        return tuple(obj['items'])\n",
    "    elif '__ndarray__' in obj:\n",
    "        return np.array(obj['n_component'])\n",
    "    return obj\n",
    "\n",
    "with open(\"convex_TempTopP.json\", \"r\") as file:\n",
    "    dict_1_convex_json = json.load(file)\n",
    "\n",
    "# Convert tuples and NumPy arrays back to original format\n",
    "dict_1_convex_json = {key: (value[0], value[1]['n_component']) for key, value in dict_1_convex_json.items()}\n",
    "\n",
    "# we can clean it up further by summing all the values in the n_components to get the sum of information retained\n",
    "dict_1_convex_json = {key: (value[0], sum(value[1])) for key, value in dict_1_convex_json.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6c3dcb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note order goes (TopP=0|Temperature=0, TopP=0.5|Temperature=0, TopP=1|Temperature=0, TopP=0|Temperature=1, TopP=0.5|Temperature=1, TopP=1|Temperature=1, TopP=0|Temperature=2, TopP=0.5|Temperature=2, Human 50 v1, Human 50 v2)\n",
      "\n",
      "Percent difference for ablation_tempTopP_froth.csv:[0, 6, 3, 3, -14, -17, 7, -4, -25, -26]\n",
      "Percent difference for ablation_tempTopP_towels.csv:[0, -9, 8, -1, -1, -17, -2, -11, -31, -29]\n",
      "Percent difference for ablation_tempTopP_time.csv:[0, -5, 13, -3, 6, -21, 19, 10, -49, -48]\n",
      "Percent difference for ablation_tempTopP_powder.csv:[0, -5, 5, -10, 0, -31, -13, 6, -33, -37]\n",
      "Percent difference for ablation_tempTopP_exercise.csv:[0, 5, -6, -12, -18, -26, -6, -17, -39, -43]\n"
     ]
    }
   ],
   "source": [
    "# DPP percent difference calculation\n",
    "percent_diff = {}\n",
    "for key, value in dict_1_DPP_json.items():\n",
    "    csv_file = key[0]\n",
    "    if csv_file not in percent_diff:\n",
    "        percent_diff[csv_file] = []\n",
    "    percent_diff[csv_file].append(value)\n",
    "\n",
    "print(\"Note order goes (TopP=0|Temperature=0, TopP=0.5|Temperature=0, TopP=1|Temperature=0, TopP=0|Temperature=1, TopP=0.5|Temperature=1, TopP=1|Temperature=1, TopP=0|Temperature=2, TopP=0.5|Temperature=2, Human 50 v1, Human 50 v2)\" )\n",
    "print(\"\")\n",
    "for csv_file, values in percent_diff.items():\n",
    "    percent_changes = percent_change(values)\n",
    "    print(f\"Percent difference for {csv_file}:{percent_changes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cd4aeef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note order goes (TopP=0|Temperature=0, TopP=0.5|Temperature=0, TopP=1|Temperature=0, TopP=0|Temperature=1, TopP=0.5|Temperature=1, TopP=1|Temperature=1, TopP=0|Temperature=2, TopP=0.5|Temperature=2, Human 50 v1, Human 50 v2)\n",
      "\n",
      "Percent difference for ablation_tempTopP_froth.csv:[0, -1, -1, -4, -2, 8, 4, -4, 20, 23]\n",
      "Percent difference for ablation_tempTopP_towels.csv:[0, -1, -16, 3, 8, 23, 5, 11, 65, 61]\n",
      "Percent difference for ablation_tempTopP_time.csv:[0, 0, -6, -5, -11, -2, -19, -9, 18, 16]\n",
      "Percent difference for ablation_tempTopP_powder.csv:[0, 10, -2, 12, 8, 25, 13, -11, 35, 35]\n",
      "Percent difference for ablation_tempTopP_exercise.csv:[0, 1, 2, 5, 4, 5, 2, 4, 7, 10]\n"
     ]
    }
   ],
   "source": [
    "# Nearest Generated Difference percent difference calculation\n",
    "percent_diff = {}\n",
    "for key, value in dict_1_nearest_json.items():\n",
    "    csv_file = key[0]\n",
    "    if csv_file not in percent_diff:\n",
    "        percent_diff[csv_file] = []\n",
    "    percent_diff[csv_file].append(value)\n",
    "\n",
    "print(\"Note order goes (TopP=0|Temperature=0, TopP=0.5|Temperature=0, TopP=1|Temperature=0, TopP=0|Temperature=1, TopP=0.5|Temperature=1, TopP=1|Temperature=1, TopP=0|Temperature=2, TopP=0.5|Temperature=2, Human 50 v1, Human 50 v2)\" )\n",
    "print(\"\")\n",
    "for csv_file, values in percent_diff.items():\n",
    "    percent_changes = percent_change(values)\n",
    "    print(f\"Percent difference for {csv_file}:{percent_changes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2292acb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note order goes (TopP=0|Temperature=0, TopP=0.5|Temperature=0, TopP=1|Temperature=0, TopP=0|Temperature=1, TopP=0.5|Temperature=1, TopP=1|Temperature=1, TopP=0|Temperature=2, TopP=0.5|Temperature=2, Human 50 v1, Human 50 v2)\n",
      "\n",
      "Percent difference for ablation_tempTopP_froth.csv:[0, 12, 1, -34, -43, 109, 141, -45, 1097, 1704]\n",
      "Percent difference for ablation_tempTopP_towels.csv:[0, 13, -84, 105, 234, 1438, 71, 280, 124959, 99300]\n",
      "Percent difference for ablation_tempTopP_time.csv:[0, 21, -50, -45, -73, -35, -90, -60, 1110, 1280]\n",
      "Percent difference for ablation_tempTopP_powder.csv:[0, 221, -1, 465, 278, 2396, 482, -69, 11808, 8956]\n",
      "Percent difference for ablation_tempTopP_exercise.csv:[0, 25, 28, 146, 66, 111, 32, 46, 215, 290]\n"
     ]
    }
   ],
   "source": [
    "# Convex Hull percent difference calculation\n",
    "percent_diff = {}\n",
    "for key, value in dict_1_convex_json.items():\n",
    "    key = eval(key)\n",
    "    csv_file = key[0]\n",
    "    value1 = value[0]\n",
    "    if csv_file not in percent_diff:\n",
    "        percent_diff[csv_file] = []\n",
    "    percent_diff[csv_file].append(value1)\n",
    "\n",
    "print(\"Note order goes (TopP=0|Temperature=0, TopP=0.5|Temperature=0, TopP=1|Temperature=0, TopP=0|Temperature=1, TopP=0.5|Temperature=1, TopP=1|Temperature=1, TopP=0|Temperature=2, TopP=0.5|Temperature=2, Human 50 v1, Human 50 v2)\" )\n",
    "print(\"\")\n",
    "for csv_file, values in percent_diff.items():\n",
    "    percent_changes = percent_change(values)\n",
    "    print(f\"Percent difference for {csv_file}:{percent_changes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9c784576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note order goes (TopP=0|Temperature=0, TopP=0.5|Temperature=0, TopP=1|Temperature=0, TopP=0|Temperature=1, TopP=0.5|Temperature=1, TopP=1|Temperature=1, TopP=0|Temperature=2, TopP=0.5|Temperature=2, Human 50 v1, Human 50 v2)\n",
      "\n",
      "Percent difference for ablation_tempTopP_froth.csv:[0, -1, -1, -4, -5, 6, 6, -5, 17, 21]\n",
      "Percent difference for ablation_tempTopP_towels.csv:[0, -4, -16, 3, 8, 19, 6, 9, 65, 61]\n",
      "Percent difference for ablation_tempTopP_time.csv:[0, -1, -5, -6, -11, -6, -20, -10, 15, 14]\n",
      "Percent difference for ablation_tempTopP_powder.csv:[0, 11, -2, 14, 11, 25, 13, -10, 36, 35]\n",
      "Percent difference for ablation_tempTopP_exercise.csv:[0, 2, 2, 5, 2, 2, 2, 3, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# Centroid Distance percent difference calculation\n",
    "percent_diff = {}\n",
    "for key, value in dict_1_centroid_json.items():\n",
    "    csv_file = key[0]\n",
    "    if csv_file not in percent_diff:\n",
    "        percent_diff[csv_file] = []\n",
    "    percent_diff[csv_file].append(value)\n",
    "\n",
    "print(\"Note order goes (TopP=0|Temperature=0, TopP=0.5|Temperature=0, TopP=1|Temperature=0, TopP=0|Temperature=1, TopP=0.5|Temperature=1, TopP=1|Temperature=1, TopP=0|Temperature=2, TopP=0.5|Temperature=2, Human 50 v1, Human 50 v2)\" )\n",
    "print(\"\")\n",
    "for csv_file, values in percent_diff.items():\n",
    "    percent_changes = percent_change(values)\n",
    "    print(f\"Percent difference for {csv_file}:{percent_changes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ae432",
   "metadata": {},
   "source": [
    "## RQ2:\n",
    "\n",
    "(1b) How does styling of the input prompt impact the output quality and diversity of the prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3bbf488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of CSV files\n",
    "csv_files = [\n",
    "    'data/ablation_topic_towels.csv',\n",
    "    'data/ablation_topic_powder.csv',\n",
    "    'data/ablation_topic_time.csv',\n",
    "    'data/ablation_topic_exercise.csv',\n",
    "    'data/ablation_topic_froth.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22055476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this only needs to be ran once.\n",
    "dict_2_DPP = {}\n",
    "dict_2_convex = {}\n",
    "dict_2_centroid = {}\n",
    "dict_2_nearest = {}\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    count = 0\n",
    "    for column in df.columns:\n",
    "        # encode the column text data into embeddings\n",
    "        embeddings = model.encode(df[column].astype(str).tolist())\n",
    "        # calculate the DPP\n",
    "        dict_2_DPP[(csv_file, count)] = DPP_diversity(embeddings, lambda0=0.1)\n",
    "        # calculate the convex hull\n",
    "        dict_2_convex[(csv_file, count)] = convexhull(embeddings, n_components = 13)\n",
    "        # calculate the distance to centroid\n",
    "        dict_2_centroid[(csv_file, count)] = distance_to_centroid(embeddings)\n",
    "        # calculate the nearest generated distance (average)\n",
    "        dict_2_nearest[(csv_file, count)] = gen_gen_distance(embeddings, reduction = \"ave\")\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52b4fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPP\n",
    "import json\n",
    "# convert tuples in dictionary to strings\n",
    "dict_2_DPP_str = {str(key): value for key, value in dict_2_DPP.items()}\n",
    "\n",
    "# convert float32 values to float\n",
    "dict_2_DPP_str = {key:float(value) for key, value in dict_2_DPP_str.items()}\n",
    "\n",
    "with open(\"DPP_Topics.json\", \"w\") as file:\n",
    "    json.dump(dict_2_DPP_str, file)\n",
    "    \n",
    "# Centroid Distance\n",
    "dict_2_centroid_str = {str(key): value for key, value in dict_2_centroid.items()}\n",
    "\n",
    "dict_2_centroid_str = {key:float(value) for key, value in dict_2_centroid_str.items()}\n",
    "\n",
    "with open(\"centroid_Topics.json\", \"w\") as file:\n",
    "    json.dump(dict_2_centroid_str, file)\n",
    "    \n",
    "# Nearest Generated Sample\n",
    "dict_2_nearest_str = {str(key): value for key, value in dict_2_nearest.items()}\n",
    "\n",
    "dict_2_nearest_str = {key:float(value) for key, value in dict_2_nearest_str.items()}\n",
    "\n",
    "with open(\"nearest_Topics.json\", \"w\") as file:\n",
    "    json.dump(dict_2_nearest_str, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90438916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convex Hull\n",
    "def convert_to_json(obj):\n",
    "    if isinstance(obj, tuple):\n",
    "        return {'__tuple__': True, 'items': list(obj)}\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return {'__ndarray__': True, 'n_component': obj.tolist()}\n",
    "    return obj\n",
    "\n",
    "\n",
    "dict_2_convex_str = {str(key): value for key, value in dict_2_convex.items()}\n",
    "\n",
    "with open(\"convex_Topics.json\", \"w\") as file:\n",
    "    json.dump(dict_2_convex_str, file, default = convert_to_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dca3b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to retrieve the json file\n",
    "with open(\"DPP_Topics.json\", \"r\") as file:\n",
    "    dict_2_DPP_json = json.load(file)\n",
    "dict_2_DPP_json = {eval(key): value for key, value in dict_2_DPP_json.items()}\n",
    "\n",
    "with open(\"centroid_Topics.json\", \"r\") as file:\n",
    "    dict_2_centroid_json = json.load(file)\n",
    "dict_2_centroid_json = {eval(key): value for key, value in dict_2_centroid_json.items()}\n",
    "\n",
    "with open(\"nearest_Topics.json\", \"r\") as file:\n",
    "    dict_2_nearest_json = json.load(file)\n",
    "dict_2_nearest_json = {eval(key): value for key, value in dict_2_nearest_json.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1289f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert convex hull back to the original dictionary with tuples and numpy array, we can use a custom decoder function\n",
    "\n",
    "def custom_decoder(obj):\n",
    "    if '__tuple__' in obj:\n",
    "        return tuple(obj['items'])\n",
    "    elif '__ndarray__' in obj:\n",
    "        return np.array(obj['n_component'])\n",
    "    return obj\n",
    "\n",
    "with open(\"convex_Topics.json\", \"r\") as file:\n",
    "    dict_2_convex_json = json.load(file)\n",
    "\n",
    "# Convert tuples and NumPy arrays back to original format\n",
    "dict_2_convex_json = {key: (value[0], value[1]['n_component']) for key, value in dict_2_convex_json.items()}\n",
    "\n",
    "# we can clean it up further by summing all the values in the n_components to get the sum of information retained\n",
    "dict_2_convex_json = {key: (value[0], sum(value[1])) for key, value in dict_2_convex_json.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c81e805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note order goes zero-shot, few-shot, novel, unique, creative, human-1, human-2\n",
      "\n",
      "Percent difference for data/ablation_topic_towels.csv:[0, -16, -9, 5, -22, -17, -15]\n",
      "Percent difference for data/ablation_topic_powder.csv:[0, 25, 20, 20, 1, -4, -9]\n",
      "Percent difference for data/ablation_topic_time.csv:[0, -23, -13, 5, 13, -35, -34]\n",
      "Percent difference for data/ablation_topic_exercise.csv:[0, -7, -20, 8, -13, -18, -23]\n",
      "Percent difference for data/ablation_topic_froth.csv:[0, 3, -6, -19, -8, -7, -7]\n"
     ]
    }
   ],
   "source": [
    "# DPP percent difference calculation\n",
    "percent_diff = {}\n",
    "for key, value in dict_2_DPP_json.items():\n",
    "    csv_file = key[0]\n",
    "    if csv_file not in percent_diff:\n",
    "        percent_diff[csv_file] = []\n",
    "    percent_diff[csv_file].append(value)\n",
    "\n",
    "print(\"Note order goes zero-shot, few-shot, novel, unique, creative, human-1, human-2\")\n",
    "print(\"\")\n",
    "for csv_file, values in percent_diff.items():\n",
    "    percent_changes = percent_change(values)\n",
    "    print(f\"Percent difference for {csv_file}:{percent_changes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aff1fdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note order goes zero-shot, few-shot, novel, unique, creative, human-1, human-2\n",
      "\n",
      "Percent difference for data/ablation_topic_towels.csv:[0, 12, 8, -1, 18, 34, 31]\n",
      "Percent difference for data/ablation_topic_powder.csv:[0, -5, -3, -4, -1, 8, 7]\n",
      "Percent difference for data/ablation_topic_time.csv:[0, 8, 6, 6, 1, 21, 19]\n",
      "Percent difference for data/ablation_topic_exercise.csv:[0, 0, 1, -1, 0, 1, 4]\n",
      "Percent difference for data/ablation_topic_froth.csv:[0, -1, 0, -2, 0, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# Nearest percent difference calculation\n",
    "percent_diff = {}\n",
    "for key, value in dict_2_nearest_json.items():\n",
    "    csv_file = key[0]\n",
    "    if csv_file not in percent_diff:\n",
    "        percent_diff[csv_file] = []\n",
    "    percent_diff[csv_file].append(value)\n",
    "\n",
    "print(\"Note order goes zero-shot, few-shot, novel, unique, creative, human-1, human-2\")\n",
    "print(\"\")\n",
    "for csv_file, values in percent_diff.items():\n",
    "    percent_changes = percent_change(values)\n",
    "    print(f\"Percent difference for {csv_file}:{percent_changes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca2f8b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note order goes zero-shot, few-shot, novel, unique, creative, human-1, human-2\n",
      "\n",
      "Percent difference for data/ablation_topic_towels.csv:[0, 291, 242, 27, 905, 8030, 6362]\n",
      "Percent difference for data/ablation_topic_powder.csv:[0, -40, -17, -28, -16, 377, 263]\n",
      "Percent difference for data/ablation_topic_time.csv:[0, 235, 205, 192, 14, 1769, 2032]\n",
      "Percent difference for data/ablation_topic_exercise.csv:[0, 0, 2, -17, -15, 49, 85]\n",
      "Percent difference for data/ablation_topic_froth.csv:[0, 19, 2, 147, -17, 375, 610]\n"
     ]
    }
   ],
   "source": [
    "# Convex Hull percent difference calculation\n",
    "percent_diff = {}\n",
    "for key, value in dict_2_convex_json.items():\n",
    "    key = eval(key)\n",
    "    csv_file = key[0]\n",
    "    value1 = value[0]\n",
    "    if csv_file not in percent_diff:\n",
    "        percent_diff[csv_file] = []\n",
    "    percent_diff[csv_file].append(value1)\n",
    "\n",
    "print(\"Note order goes zero-shot, few-shot, novel, unique, creative, human-1, human-2\")\n",
    "print(\"\")\n",
    "for csv_file, values in percent_diff.items():\n",
    "    percent_changes = percent_change(values)\n",
    "    print(f\"Percent difference for {csv_file}:{percent_changes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc98957b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note order goes zero-shot, few-shot, novel, unique, creative, human-1, human-2\n",
      "\n",
      "Percent difference for data/ablation_topic_towels.csv:[0, 11, 8, 0, 18, 39, 36]\n",
      "Percent difference for data/ablation_topic_powder.csv:[0, -3, -3, -3, 0, 9, 8]\n",
      "Percent difference for data/ablation_topic_time.csv:[0, 8, 7, 7, 1, 23, 21]\n",
      "Percent difference for data/ablation_topic_exercise.csv:[0, -1, 2, -1, 0, 2, 3]\n",
      "Percent difference for data/ablation_topic_froth.csv:[0, -1, -1, -8, -1, 5, 8]\n"
     ]
    }
   ],
   "source": [
    "# Centroid Distance percent difference calculation\n",
    "percent_diff = {}\n",
    "for key, value in dict_2_centroid_json.items():\n",
    "    csv_file = key[0]\n",
    "    if csv_file not in percent_diff:\n",
    "        percent_diff[csv_file] = []\n",
    "    percent_diff[csv_file].append(value)\n",
    "\n",
    "print(\"Note order goes zero-shot, few-shot, novel, unique, creative, human-1, human-2\")\n",
    "print(\"\")\n",
    "for csv_file, values in percent_diff.items():\n",
    "    percent_changes = percent_change(values)\n",
    "    print(f\"Percent difference for {csv_file}:{percent_changes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea7de33",
   "metadata": {},
   "source": [
    "## RQ3:\n",
    "\n",
    "(1c) Do different design matters impact the output quality of the prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af1c3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to retrieve the json file\n",
    "with open(\"DPP_Topics.json\", \"r\") as file:\n",
    "    dict_2_DPP_json = json.load(file)\n",
    "dict_2_DPP_json = {eval(key): value for key, value in dict_2_DPP_json.items()}\n",
    "\n",
    "with open(\"centroid_Topics.json\", \"r\") as file:\n",
    "    dict_2_centroid_json = json.load(file)\n",
    "dict_2_centroid_json = {eval(key): value for key, value in dict_2_centroid_json.items()}\n",
    "\n",
    "with open(\"nearest_Topics.json\", \"r\") as file:\n",
    "    dict_2_nearest_json = json.load(file)\n",
    "dict_2_nearest_json = {eval(key): value for key, value in dict_2_nearest_json.items()}\n",
    "\n",
    "# to convert convex hull back to the original dictionary with tuples and numpy array, we can use a custom decoder function\n",
    "\n",
    "def custom_decoder(obj):\n",
    "    if '__tuple__' in obj:\n",
    "        return tuple(obj['items'])\n",
    "    elif '__ndarray__' in obj:\n",
    "        return np.array(obj['n_component'])\n",
    "    return obj\n",
    "\n",
    "with open(\"convex_TempTopP.json\", \"r\") as file:\n",
    "    dict_1_convex_json = json.load(file)\n",
    "\n",
    "# Convert tuples and NumPy arrays back to original format\n",
    "dict_1_convex_json = {key: (value[0], value[1]['n_component']) for key, value in dict_1_convex_json.items()}\n",
    "\n",
    "# we can clean it up further by summing all the values in the n_components to get the sum of information retained\n",
    "dict_1_convex_json = {key: (value[0], sum(value[1])) for key, value in dict_1_convex_json.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1aa3ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work in progress to calculate it all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c40a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:research]",
   "language": "python",
   "name": "conda-env-research-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
